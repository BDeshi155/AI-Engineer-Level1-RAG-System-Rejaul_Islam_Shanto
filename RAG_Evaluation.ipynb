{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01xcNn_OwTtm",
        "outputId": "4afaa590-0a3f-4c58-ae1d-ede00cd0f0e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.71)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (0.3.71)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (1.0.15)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.73.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.11.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (0.4.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.56b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.33.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
            "Initializing multilingual embeddings...\n",
            "Initializing language model...\n",
            "Setting up Enhanced RAG Pipeline with Evaluation...\n",
            "Loading documents from: /content/drive/MyDrive/dataset/HSC26_chunks.txt\n",
            "Loaded 141 document chunks\n",
            "Creating vector store...\n",
            "Vector store created successfully!\n",
            "Setting up QA chain...\n",
            "QA chain setup complete!\n",
            "Enhanced RAG Pipeline setup complete!\n",
            "\n",
            "ðŸ”¬ COMPREHENSIVE RAG EVALUATION\n",
            "============================================================\n",
            "Running batch evaluation on 3 test cases...\n",
            "Evaluating test case 1/3\n",
            "Processing query: à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\n",
            "Evaluating test case 2/3\n",
            "Processing query: à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\n",
            "Evaluating test case 3/3\n",
            "Processing query: à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤ à¦›à¦¿à¦²?\n",
            "\n",
            "=== RAG SYSTEM EVALUATION REPORT ===\n",
            "Generated on: 2025-07-26 08:21:26\n",
            "\n",
            "ðŸ“Š AGGREGATE METRICS:\n",
            "- Average Groundedness Score: 0.677\n",
            "- Average Relevance Score: 0.849\n",
            "- Average Answer Accuracy: 0.333\n",
            "\n",
            "ðŸ“‹ DETAILED RESULTS:\n",
            "\n",
            "Test Case 1:\n",
            "Question: à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\n",
            "Expected: à¦¶à§à¦®à§à¦­à¦¨à¦¾à¦¥\n",
            "Got: â€˜à¦…à¦ªà¦°à¦¿à¦šà¦¿à¦¤à¦¾â€™ à¦®à¦¨à¦¸à§à¦¤à¦¾à¦ªà§‡ à¦­à§‡à¦™à§à¦—à§‡à¦ªà§œà¦¾ à¦à¦• à¦¬à§à¦¯à¦•à§à¦¤à§€à¦¤à§à¦¬à¦¹à§€à¦¨ à¦¯à§à¦¬à¦•à§‡à¦° à¦¸à§à¦¬à§€à¦•à¦¾à¦°à§‹à¦•à§à¦¤à¦¿à¦° à¦—à¦²à§à¦ª, à¦¤à¦¾à¦° à¦ªà¦¾\n",
            "Groundedness: 0.700\n",
            "Relevance: 0.843\n",
            "Accuracy: 0.428\n",
            "==================================================\n",
            "\n",
            "Test Case 2:\n",
            "Question: à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\n",
            "Expected: à¦®à¦¾à¦®à§‹\n",
            "Got: à¦ à¦¯à¦œà§à¦žà§‡ à¦ªà¦¤à¦¿à¦¨à¦¿à¦¨à§à¦¦à¦¾ à¦¶à§à¦¨à§‡ à¦¸à¦¤à§€ à¦¦à§‡à¦¹à¦¤à§à¦¯à¦¾à¦— à¦•à¦°à§‡à¦¨\n",
            "Groundedness: 0.665\n",
            "Relevance: 0.861\n",
            "Accuracy: 0.344\n",
            "==================================================\n",
            "\n",
            "Test Case 3:\n",
            "Question: à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤ à¦›à¦¿à¦²?\n",
            "Expected: à§§à§« à¦¬à¦›à¦°\n",
            "Got: à¦ à¦¯à¦œà§à¦žà§‡ à¦ªà¦¤à¦¿à¦¨à¦¿à¦¨à§à¦¦à¦¾ à¦¶à§à¦¨à§‡ à¦¸à¦¤à§€ à¦¦à§‡à¦¹à¦¤à§à¦¯à¦¾à¦— à¦•à¦°à§‡à¦¨\n",
            "Groundedness: 0.665\n",
            "Relevance: 0.844\n",
            "Accuracy: 0.226\n",
            "==================================================\n",
            "\n",
            "\n",
            "ðŸ“ˆ EVALUATION METRICS SUMMARY:\n",
            "       groundedness_score  relevance_score  accuracy_score\n",
            "count            3.000000         3.000000        3.000000\n",
            "mean             0.676643         0.849376        0.332566\n",
            "std              0.020298         0.010504        0.101521\n",
            "min              0.664923         0.842681        0.225894\n",
            "25%              0.664923         0.843323        0.284849\n",
            "50%              0.664923         0.843966        0.343804\n",
            "75%              0.682502         0.852724        0.385903\n",
            "max              0.700081         0.861482        0.428001\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain-community\n",
        "%pip install -U langchain-chroma\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms.base import LLM\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"Class to store evaluation results\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    expected_answer: str\n",
        "    retrieved_contexts: List[str]\n",
        "    groundedness_score: float\n",
        "    relevance_score: float\n",
        "    cosine_similarity_score: float\n",
        "    human_judgment: Optional[str] = None\n",
        "    timestamp: str = None\n",
        "\n",
        "class HuggingFaceLLM(LLM):\n",
        "    \"\"\"Custom LLM wrapper for Hugging Face Inference API\"\"\"\n",
        "    model_name: str = \"microsoft/DialoGPT-medium\"\n",
        "    api_url: str = \"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Make API call to Hugging Face\"\"\"\n",
        "        api_url = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n",
        "\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 150,\n",
        "                \"temperature\": 0.1,\n",
        "                \"return_full_text\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    return result[0].get('generated_text', '').strip()\n",
        "                elif isinstance(result, dict):\n",
        "                    return result.get('generated_text', '').strip()\n",
        "            else:\n",
        "                return self._simple_answer_extraction(prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"API Error: {e}\")\n",
        "            return self._simple_answer_extraction(prompt)\n",
        "\n",
        "    def _simple_answer_extraction(self, prompt: str) -> str:\n",
        "        \"\"\"Simple fallback method to extract answers from context\"\"\"\n",
        "        if \"Question:\" in prompt and \"Context:\" in prompt:\n",
        "            context_start = prompt.find(\"Context:\") + 8\n",
        "            question_start = prompt.find(\"Question:\") + 9\n",
        "            context = prompt[context_start:prompt.find(\"Question:\")].strip()\n",
        "            question = prompt[question_start:].strip()\n",
        "\n",
        "            question_lower = question.lower()\n",
        "            context_lines = context.split('\\n')\n",
        "\n",
        "            for line in context_lines:\n",
        "                if any(word in line for word in question.split() if len(word) > 2):\n",
        "                    words = line.split()\n",
        "                    if len(words) > 0:\n",
        "                        return line.strip()[:100]\n",
        "\n",
        "        return \"à¦¤à¦¥à§à¦¯ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼à¦¨à¦¿\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"huggingface\"\n",
        "\n",
        "class RAGEvaluator:\n",
        "    \"\"\"Class to evaluate RAG system performance\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings_model):\n",
        "        self.embeddings_model = embeddings_model\n",
        "        self.evaluation_results = []\n",
        "\n",
        "    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
        "        try:\n",
        "            # Get embeddings for both texts\n",
        "            emb1 = self.embeddings_model.embed_query(text1)\n",
        "            emb2 = self.embeddings_model.embed_query(text2)\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
        "            return float(similarity)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating cosine similarity: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def evaluate_groundedness(self, answer: str, contexts: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate if the answer is grounded in the retrieved contexts\n",
        "        Returns a score between 0 and 1\n",
        "        \"\"\"\n",
        "        if not answer or not contexts:\n",
        "            return 0.0\n",
        "\n",
        "        combined_context = \" \".join(contexts)\n",
        "\n",
        "        answer_words = set(answer.lower().split())\n",
        "        context_words = set(combined_context.lower().split())\n",
        "\n",
        "        overlap = len(answer_words.intersection(context_words))\n",
        "        total_answer_words = len(answer_words)\n",
        "\n",
        "        if total_answer_words == 0:\n",
        "            return 0.0\n",
        "\n",
        "        word_overlap_score = overlap / total_answer_words\n",
        "\n",
        "        cosine_score = self.calculate_cosine_similarity(answer, combined_context)\n",
        "\n",
        "        pattern_score = self._evaluate_bengali_patterns(answer, combined_context)\n",
        "\n",
        "        groundedness_score = (\n",
        "            0.4 * word_overlap_score +\n",
        "            0.4 * cosine_score +\n",
        "            0.2 * pattern_score\n",
        "        )\n",
        "\n",
        "        return min(1.0, groundedness_score)\n",
        "\n",
        "    def _evaluate_bengali_patterns(self, answer: str, context: str) -> float:\n",
        "        \"\"\"Evaluate Bengali-specific patterns\"\"\"\n",
        "        bengali_numbers = ['à§§', 'à§¨', 'à§©', 'à§ª', 'à§«', 'à§¬', 'à§­', 'à§®', 'à§¯', 'à§¦']\n",
        "        bengali_words = ['à¦¬à¦›à¦°', 'à¦¸à¦¾à¦²', 'à¦®à¦¾à¦¸', 'à¦¦à¦¿à¦¨']\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        for num in bengali_numbers:\n",
        "            if num in answer and num in context:\n",
        "                score += 0.2\n",
        "\n",
        "        for word in bengali_words:\n",
        "            if word in answer and word in context:\n",
        "                score += 0.1\n",
        "\n",
        "        return min(1.0, score)\n",
        "\n",
        "    def evaluate_relevance(self, question: str, retrieved_docs: List[Document]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate the relevance of retrieved documents to the question\n",
        "        Returns a score between 0 and 1\n",
        "        \"\"\"\n",
        "        if not question or not retrieved_docs:\n",
        "            return 0.0\n",
        "\n",
        "        relevance_scores = []\n",
        "\n",
        "        for doc in retrieved_docs:\n",
        "            doc_relevance = self.calculate_cosine_similarity(question, doc.page_content)\n",
        "            relevance_scores.append(doc_relevance)\n",
        "\n",
        "        return np.mean(relevance_scores) if relevance_scores else 0.0\n",
        "\n",
        "    def evaluate_answer_accuracy(self, predicted_answer: str, expected_answer: str) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate how accurately the predicted answer matches the expected answer\n",
        "        \"\"\"\n",
        "        if not predicted_answer or not expected_answer:\n",
        "            return 0.0\n",
        "\n",
        "        cosine_score = self.calculate_cosine_similarity(predicted_answer, expected_answer)\n",
        "\n",
        "        if expected_answer.lower() in predicted_answer.lower():\n",
        "            exact_match_score = 1.0\n",
        "        else:\n",
        "            exact_match_score = 0.0\n",
        "\n",
        "        accuracy_score = 0.6 * cosine_score + 0.4 * exact_match_score\n",
        "\n",
        "        return accuracy_score\n",
        "\n",
        "    def comprehensive_evaluation(self, question: str, answer: str, expected_answer: str,\n",
        "                               retrieved_docs: List[Document]) -> EvaluationResult:\n",
        "        \"\"\"Perform comprehensive evaluation of a single query\"\"\"\n",
        "\n",
        "        contexts = [doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "        groundedness_score = self.evaluate_groundedness(answer, contexts)\n",
        "        relevance_score = self.evaluate_relevance(question, retrieved_docs)\n",
        "        accuracy_score = self.evaluate_answer_accuracy(answer, expected_answer)\n",
        "\n",
        "        result = EvaluationResult(\n",
        "            question=question,\n",
        "            answer=answer,\n",
        "            expected_answer=expected_answer,\n",
        "            retrieved_contexts=contexts,\n",
        "            groundedness_score=groundedness_score,\n",
        "            relevance_score=relevance_score,\n",
        "            cosine_similarity_score=accuracy_score,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "        self.evaluation_results.append(result)\n",
        "        return result\n",
        "\n",
        "class EnhancedBilingualRAGPipeline:\n",
        "    \"\"\"Enhanced RAG Pipeline with evaluation capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        self.file_path = file_path\n",
        "        self.vectorstore = None\n",
        "        self.retriever = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "        print(\"Initializing multilingual embeddings...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "        print(\"Initializing language model...\")\n",
        "        self.llm = HuggingFaceLLM()\n",
        "\n",
        "        # Initialize evaluator\n",
        "        self.evaluator = RAGEvaluator(self.embeddings)\n",
        "\n",
        "    def load_and_prepare_documents(self) -> List[Document]:\n",
        "        \"\"\"Load documents from file\"\"\"\n",
        "        print(f\"Loading documents from: {self.file_path}\")\n",
        "        documents = []\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            # Split content by separator\n",
        "            chunks = content.split('---')\n",
        "            chunks = [chunk.strip() for chunk in chunks if chunk.strip() and len(chunk.strip()) > 10]\n",
        "\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                documents.append(Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata={\n",
        "                        'chunk_id': idx,\n",
        "                        'source': f\"chunk_{idx}\",\n",
        "                        'length': len(chunk)\n",
        "                    }\n",
        "                ))\n",
        "\n",
        "            print(f\"Loaded {len(documents)} document chunks\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file: {e}\")\n",
        "            return []\n",
        "\n",
        "    def create_vectorstore(self, documents: List[Document]):\n",
        "        \"\"\"Create vector store from documents\"\"\"\n",
        "        print(\"Creating vector store...\")\n",
        "\n",
        "        if not documents:\n",
        "            print(\"No documents to process!\")\n",
        "            return\n",
        "\n",
        "        self.vectorstore = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=\"/content/chroma_db\"\n",
        "        )\n",
        "\n",
        "        self.retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "\n",
        "        print(\"Vector store created successfully!\")\n",
        "\n",
        "    def setup_qa_chain(self):\n",
        "        \"\"\"Set up QA chain\"\"\"\n",
        "        print(\"Setting up QA chain...\")\n",
        "\n",
        "        prompt_template = \"\"\"Based on the following context, answer the question concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a direct, short answer. If the question is in Bengali, answer in Bengali. If in English, answer in English.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.retriever,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT},\n",
        "            return_source_documents=True\n",
        "        )\n",
        "\n",
        "        print(\"QA chain setup complete!\")\n",
        "\n",
        "    def query_with_evaluation(self, question: str, expected_answer: str = None) -> Dict:\n",
        "        \"\"\"Query with comprehensive evaluation\"\"\"\n",
        "        if not self.qa_chain:\n",
        "            raise ValueError(\"QA chain not initialized. Run setup() first.\")\n",
        "\n",
        "        print(f\"Processing query: {question}\")\n",
        "\n",
        "        try:\n",
        "            relevant_docs = self.retriever.invoke(question)\n",
        "\n",
        "            answer = self._extract_answer_from_context(question, relevant_docs)\n",
        "\n",
        "            evaluation_result = None\n",
        "            if expected_answer:\n",
        "                evaluation_result = self.evaluator.comprehensive_evaluation(\n",
        "                    question, answer, expected_answer, relevant_docs\n",
        "                )\n",
        "\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"expected_answer\": expected_answer,\n",
        "                \"source_documents\": relevant_docs,\n",
        "                \"evaluation\": evaluation_result\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query: {e}\")\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": \"à¦‰à¦¤à§à¦¤à¦° à¦–à§à¦à¦œà§‡ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼à¦¨à¦¿\",\n",
        "                \"expected_answer\": expected_answer,\n",
        "                \"source_documents\": [],\n",
        "                \"evaluation\": None\n",
        "            }\n",
        "\n",
        "    def _extract_answer_from_context(self, question: str, docs: List[Document]) -> str:\n",
        "        \"\"\"Enhanced answer extraction with better pattern matching\"\"\"\n",
        "        context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "        if any(word in question for word in [\"à¦•à¦¾à¦°\", \"à¦•à§‹à¦¨\", \"à¦•à¦¿\", \"à¦•à¦¤\", \"à¦•à§‡\", \"à¦•à§‹à¦¥à¦¾à¦¯à¦¼\", \"à¦•à¦–à¦¨\"]):\n",
        "            lines = context.split('à¥¤')\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "\n",
        "                if \"à¦¬à¦¯à¦¼à¦¸\" in question:\n",
        "                    age_match = re.search(r'(\\d+|[à§¦-à§¯]+)\\s*(à¦¬à¦›à¦°|à¦¬à¦¯à¦¼à¦¸)', line)\n",
        "                    if age_match:\n",
        "                        return age_match.group(0)\n",
        "\n",
        "                if \"à¦¸à§à¦ªà§à¦°à§à¦·\" in question or \"à¦¸à¦ªà§à¦°à§à¦·\" in question:\n",
        "                    if \"à¦¶à§à¦®à§à¦­à¦¨à¦¾à¦¥\" in line:\n",
        "                        return \"à¦¶à§à¦®à§à¦­à¦¨à¦¾à¦¥\"\n",
        "\n",
        "                if \"à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾\" in question or \"à¦®à¦¾à¦®à§‹\" in question:\n",
        "                    if \"à¦®à¦¾à¦®à§‹\" in line:\n",
        "                        return \"à¦®à¦¾à¦®à§‹\"\n",
        "\n",
        "                question_words = [word for word in question.split() if len(word) > 2]\n",
        "                if any(word in line for word in question_words):\n",
        "                    if len(line) > 10:\n",
        "                        return line[:80]\n",
        "\n",
        "        sentences = context.split('à¥¤')\n",
        "        for sentence in sentences[:3]:\n",
        "            sentence = sentence.strip()\n",
        "            if len(sentence) > 5:\n",
        "                return sentence[:50]\n",
        "\n",
        "        return \"à¦¤à¦¥à§à¦¯ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼à¦¨à¦¿\"\n",
        "\n",
        "    def batch_evaluate(self, test_cases: List[Dict]) -> List[EvaluationResult]:\n",
        "        \"\"\"Evaluate multiple test cases at once\"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(f\"Running batch evaluation on {len(test_cases)} test cases...\")\n",
        "\n",
        "        for i, test_case in enumerate(test_cases):\n",
        "            print(f\"Evaluating test case {i+1}/{len(test_cases)}\")\n",
        "\n",
        "            question = test_case['question']\n",
        "            expected_answer = test_case['expected_answer']\n",
        "\n",
        "            result = self.query_with_evaluation(question, expected_answer)\n",
        "            if result['evaluation']:\n",
        "                results.append(result['evaluation'])\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_evaluation_report(self, results: List[EvaluationResult]) -> str:\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        if not results:\n",
        "            return \"No evaluation results available.\"\n",
        "\n",
        "        avg_groundedness = np.mean([r.groundedness_score for r in results])\n",
        "        avg_relevance = np.mean([r.relevance_score for r in results])\n",
        "        avg_accuracy = np.mean([r.cosine_similarity_score for r in results])\n",
        "\n",
        "        report = f\"\"\"\n",
        "=== RAG SYSTEM EVALUATION REPORT ===\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "ðŸ“Š AGGREGATE METRICS:\n",
        "- Average Groundedness Score: {avg_groundedness:.3f}\n",
        "- Average Relevance Score: {avg_relevance:.3f}\n",
        "- Average Answer Accuracy: {avg_accuracy:.3f}\n",
        "\n",
        "ðŸ“‹ DETAILED RESULTS:\n",
        "\"\"\"\n",
        "\n",
        "        for i, result in enumerate(results, 1):\n",
        "            report += f\"\"\"\n",
        "Test Case {i}:\n",
        "Question: {result.question}\n",
        "Expected: {result.expected_answer}\n",
        "Got: {result.answer}\n",
        "Groundedness: {result.groundedness_score:.3f}\n",
        "Relevance: {result.relevance_score:.3f}\n",
        "Accuracy: {result.cosine_similarity_score:.3f}\n",
        "{'='*50}\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Complete setup of the RAG pipeline\"\"\"\n",
        "        print(\"Setting up Enhanced RAG Pipeline with Evaluation...\")\n",
        "\n",
        "        documents = self.load_and_prepare_documents()\n",
        "        if not documents:\n",
        "            print(\"No documents loaded. Please check the file path.\")\n",
        "            return False\n",
        "\n",
        "        self.create_vectorstore(documents)\n",
        "        self.setup_qa_chain()\n",
        "\n",
        "        print(\"Enhanced RAG Pipeline setup complete!\")\n",
        "        return True\n",
        "\n",
        "def run_comprehensive_evaluation():\n",
        "    \"\"\"Run comprehensive evaluation of the RAG system\"\"\"\n",
        "\n",
        "    file_path = \"/content/drive/MyDrive/dataset/HSC26_chunks.txt\"\n",
        "    rag_pipeline = EnhancedBilingualRAGPipeline(file_path)\n",
        "\n",
        "    success = rag_pipeline.setup()\n",
        "    if not success:\n",
        "        print(\"Failed to setup pipeline.\")\n",
        "        return\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"question\": \"à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\",\n",
        "            \"expected_answer\": \"à¦¶à§à¦®à§à¦­à¦¨à¦¾à¦¥\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\",\n",
        "            \"expected_answer\": \"à¦®à¦¾à¦®à§‹\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤ à¦›à¦¿à¦²?\",\n",
        "            \"expected_answer\": \"à§§à§« à¦¬à¦›à¦°\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nðŸ”¬ COMPREHENSIVE RAG EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    evaluation_results = rag_pipeline.batch_evaluate(test_cases)\n",
        "\n",
        "    report = rag_pipeline.generate_evaluation_report(evaluation_results)\n",
        "    print(report)\n",
        "\n",
        "    results_data = []\n",
        "    for result in evaluation_results:\n",
        "        results_data.append({\n",
        "            'question': result.question,\n",
        "            'answer': result.answer,\n",
        "            'expected_answer': result.expected_answer,\n",
        "            'groundedness_score': result.groundedness_score,\n",
        "            'relevance_score': result.relevance_score,\n",
        "            'accuracy_score': result.cosine_similarity_score,\n",
        "            'timestamp': result.timestamp\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_data)\n",
        "    print(\"\\nðŸ“ˆ EVALUATION METRICS SUMMARY:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return rag_pipeline, evaluation_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, results = run_comprehensive_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wU-fjEwC0gCU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}